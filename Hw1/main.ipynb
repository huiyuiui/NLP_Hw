{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6yklm2xWn9f"
      },
      "source": [
        "## Part I: Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YycoIJomXwqH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KKAXuhZIUxD8"
      },
      "outputs": [],
      "source": [
        "# Download the Google Analogy dataset\n",
        "# !wget http://download.tensorflow.org/data/questions-words.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xue5pVFLVNQQ"
      },
      "outputs": [],
      "source": [
        "# Preprocess the dataset\n",
        "file_name = \"questions-words\"\n",
        "with open(f\"{file_name}.txt\", \"r\") as f:\n",
        "    data = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h7dOdJOsZzAF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ": capital-common-countries\n",
            "Athens Greece Baghdad Iraq\n",
            "Athens Greece Bangkok Thailand\n",
            "Athens Greece Beijing China\n",
            "Athens Greece Berlin Germany\n",
            "Athens Greece Bern Switzerland\n",
            "Athens Greece Cairo Egypt\n",
            "Athens Greece Canberra Australia\n",
            "Athens Greece Hanoi Vietnam\n",
            "Athens Greece Havana Cuba\n"
          ]
        }
      ],
      "source": [
        "# check data from the first 10 entries\n",
        "for entry in data[:10]:\n",
        "    print(entry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wmYQ0IWZZxf3"
      },
      "outputs": [],
      "source": [
        "# TODO1: Write your code here for processing data to pd.DataFrame\n",
        "# Please note that the first five mentions of \": \" indicate `semantic`,\n",
        "# and the remaining nine belong to the `syntatic` category.\n",
        "questions = []\n",
        "categories = []\n",
        "sub_categories = []\n",
        "now_categories = \"semeantic\"\n",
        "now_sub_categories = \"\"\n",
        "count = 0\n",
        "for line in data:\n",
        "    if(\":\" in line):\n",
        "        count += 1\n",
        "        now_sub_categories = line\n",
        "        if(count >= 6):\n",
        "            now_categories = 'syntatic'\n",
        "        continue\n",
        "    else:\n",
        "        questions.append(line)\n",
        "        categories.append(now_categories)\n",
        "        sub_categories.append(now_sub_categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_bKA05rVZb_i"
      },
      "outputs": [],
      "source": [
        "# Create the dataframe\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"Question\": questions,\n",
        "        \"Category\": categories,\n",
        "        \"SubCategory\": sub_categories,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UN2FBcicZmpV"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Category</th>\n",
              "      <th>SubCategory</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Athens Greece Baghdad Iraq</td>\n",
              "      <td>semeantic</td>\n",
              "      <td>: capital-common-countries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Athens Greece Bangkok Thailand</td>\n",
              "      <td>semeantic</td>\n",
              "      <td>: capital-common-countries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Athens Greece Beijing China</td>\n",
              "      <td>semeantic</td>\n",
              "      <td>: capital-common-countries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Athens Greece Berlin Germany</td>\n",
              "      <td>semeantic</td>\n",
              "      <td>: capital-common-countries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Athens Greece Bern Switzerland</td>\n",
              "      <td>semeantic</td>\n",
              "      <td>: capital-common-countries</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Question   Category                 SubCategory\n",
              "0      Athens Greece Baghdad Iraq  semeantic  : capital-common-countries\n",
              "1  Athens Greece Bangkok Thailand  semeantic  : capital-common-countries\n",
              "2     Athens Greece Beijing China  semeantic  : capital-common-countries\n",
              "3    Athens Greece Berlin Germany  semeantic  : capital-common-countries\n",
              "4  Athens Greece Bern Switzerland  semeantic  : capital-common-countries"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nMGvoDeiZhbp"
      },
      "outputs": [],
      "source": [
        "df.to_csv(f\"{file_name}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi2SNNuHWiZO"
      },
      "source": [
        "## Part II: Use pre-trained word embeddings\n",
        "- After finish Part I, you can run Part II code blocks only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "yB4rpJymXiSN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.downloader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-pGLoyKSHXuQ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"questions-words.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "YWa_1hF3aZHO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "The Gensim model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"glove-wiki-gigaword-100\"\n",
        "# You can try other models.\n",
        "# https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models\n",
        "\n",
        "# Load the pre-trained model (using GloVe vectors here)\n",
        "model = gensim.downloader.load(MODEL_NAME)\n",
        "print(\"The Gensim model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          Question   Category SubCategory\n",
            "5030   Algeria dinar Angola kwanza  semeantic  : currency\n",
            "5031  Algeria dinar Argentina peso  semeantic  : currency\n",
            "5032    Algeria dinar Armenia dram  semeantic  : currency\n",
            "5033     Algeria dinar Brazil real  semeantic  : currency\n",
            "5034    Algeria dinar Bulgaria lev  semeantic  : currency\n",
            "...                            ...        ...         ...\n",
            "5891     Vietnam dong Russia ruble  semeantic  : currency\n",
            "5892     Vietnam dong Sweden krona  semeantic  : currency\n",
            "5893    Vietnam dong Thailand baht  semeantic  : currency\n",
            "5894  Vietnam dong Ukraine hryvnia  semeantic  : currency\n",
            "5895       Vietnam dong USA dollar  semeantic  : currency\n",
            "\n",
            "[866 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "filtered_data = data[data['SubCategory'].str.contains('currency', case=False)]\n",
        "print(filtered_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTsqJcP1WSTH"
      },
      "outputs": [],
      "source": [
        "# Do predictions and preserve the gold answers (word_D)\n",
        "preds = []\n",
        "golds = []\n",
        "\n",
        "# for analogy in tqdm(data[\"Question\"]):\n",
        "for analogy in tqdm(filtered_data[\"Question\"]):\n",
        "      # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.\n",
        "      # You should also preserve the gold answers during iterations for evaluations later.\n",
        "      \"\"\" Hints\n",
        "      # Unpack the analogy (e.g., \"man\", \"woman\", \"king\", \"queen\")\n",
        "      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d\n",
        "      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776\n",
        "      # Mikolov et al., 2013: big - biggest and small - smallest\n",
        "      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).\n",
        "      \"\"\"\n",
        "      A, B, C, D =analogy.split(' ')\n",
        "      output = model.most_similar(positive=[B.lower(), C.lower()], negative=[A.lower()], topn=1)\n",
        "      if(D[0].isupper()): pred = output[0][0].capitalize()\n",
        "      else: pred = output[0][0]\n",
        "      print(pred)\n",
        "      preds.append(pred)\n",
        "      golds.append(D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xG7vcPXAW6uT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category: semeantic, Accuracy: 65.3399481339497%\n",
            "Category: syntatic, Accuracy: 61.255269320843084%\n",
            "Sub-Category: capital-common-countries, Accuracy: 93.87351778656127%\n",
            "Sub-Category: capital-world, Accuracy: 88.94783377541998%\n",
            "Sub-Category: currency, Accuracy: 14.203233256351039%\n",
            "Sub-Category: city-in-state, Accuracy: 30.806647750304013%\n",
            "Sub-Category: family, Accuracy: 81.62055335968378%\n",
            "Sub-Category: gram1-adjective-to-adverb, Accuracy: 24.39516129032258%\n",
            "Sub-Category: gram2-opposite, Accuracy: 20.073891625615765%\n",
            "Sub-Category: gram3-comparative, Accuracy: 79.12912912912913%\n",
            "Sub-Category: gram4-superlative, Accuracy: 54.278074866310156%\n",
            "Sub-Category: gram5-present-participle, Accuracy: 69.50757575757575%\n",
            "Sub-Category: gram6-nationality-adjective, Accuracy: 87.86741713570981%\n",
            "Sub-Category: gram7-past-tense, Accuracy: 55.44871794871795%\n",
            "Sub-Category: gram8-plural, Accuracy: 71.996996996997%\n",
            "Sub-Category: gram9-plural-verbs, Accuracy: 58.39080459770115%\n"
          ]
        }
      ],
      "source": [
        "# Perform evaluations. You do not need to modify this block!!\n",
        "\n",
        "def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:\n",
        "    return np.mean(gold == pred)\n",
        "\n",
        "golds_np, preds_np = np.array(golds), np.array(preds)\n",
        "data = pd.read_csv(\"questions-words.csv\")\n",
        "\n",
        "# Evaluation: categories\n",
        "for category in data[\"Category\"].unique():\n",
        "    mask = data[\"Category\"] == category\n",
        "    golds_cat, preds_cat = golds_np[mask], preds_np[mask]\n",
        "    acc_cat = calculate_accuracy(golds_cat, preds_cat)\n",
        "    print(f\"Category: {category}, Accuracy: {acc_cat * 100}%\")\n",
        "\n",
        "# Evaluation: sub-categories\n",
        "for sub_category in data[\"SubCategory\"].unique():\n",
        "    mask = data[\"SubCategory\"] == sub_category\n",
        "    golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]\n",
        "    acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)\n",
        "    print(f\"Sub-Category{sub_category}, Accuracy: {acc_subcat * 100}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_z6CybBXKZu"
      },
      "outputs": [],
      "source": [
        "# Collect words from Google Analogy dataset\n",
        "SUB_CATEGORY = \": family\"\n",
        "\n",
        "# TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`\n",
        "\n",
        "\n",
        "plt.title(\"Word Relationships from Google Analogy Task\")\n",
        "plt.show()\n",
        "plt.savefig(\"word_relationships.png\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKRPJxgKXH4j"
      },
      "source": [
        "### Part III: Train your own word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC_0fE1UzL8T"
      },
      "source": [
        "### Get the latest English Wikipedia articles and do sampling.\n",
        "- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkubArwCCYxR"
      },
      "outputs": [],
      "source": [
        "# Download the split Wikipedia files\n",
        "# Each file contain 562365 lines (articles).\n",
        "!gdown --id 1jiu9E1NalT2Y8EIuWNa1xf2Tw1f1XuGd -O wiki_texts_part_0.txt.gz\n",
        "!gdown --id 1ABblLRd9HXdXvaNv8H9fFq984bhnowoG -O wiki_texts_part_1.txt.gz\n",
        "!gdown --id 1z2VFNhpPvCejTP5zyejzKj5YjI_Bn42M -O wiki_texts_part_2.txt.gz\n",
        "!gdown --id 1VKjded9BxADRhIoCzXy_W8uzVOTWIf0g -O wiki_texts_part_3.txt.gz\n",
        "!gdown --id 16mBeG26m9LzHXdPe8UrijUIc6sHxhknz -O wiki_texts_part_4.txt.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S3ibNT3C8Xk"
      },
      "outputs": [],
      "source": [
        "# Download the split Wikipedia files\n",
        "# Each file contain 562365 lines (articles), except the last file.\n",
        "!gdown --id 17JFvxOH-kc-VmvGkhG7p3iSZSpsWdgJI -O wiki_texts_part_5.txt.gz\n",
        "!gdown --id 19IvB2vOJRGlrYulnTXlZECR8zT5v550P -O wiki_texts_part_6.txt.gz\n",
        "!gdown --id 1sjwO8A2SDOKruv6-8NEq7pEIuQ50ygVV -O wiki_texts_part_7.txt.gz\n",
        "!gdown --id 1s7xKWJmyk98Jbq6Fi1scrHy7fr_ellUX -O wiki_texts_part_8.txt.gz\n",
        "!gdown --id 17eQXcrvY1cfpKelLbP2BhQKrljnFNykr -O wiki_texts_part_9.txt.gz\n",
        "!gdown --id 1J5TAN6bNBiSgTIYiPwzmABvGhAF58h62 -O wiki_texts_part_10.txt.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUg_c79BC7OL"
      },
      "outputs": [],
      "source": [
        "# Extract the downloaded wiki_texts_parts files.\n",
        "!gunzip -k wiki_texts_part_*.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7duk2RbYDB02"
      },
      "outputs": [],
      "source": [
        "# Combine the extracted wiki_texts_parts files.\n",
        "!cat wiki_texts_part_*.txt > wiki_texts_combined.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "givLH7NrDs6X"
      },
      "outputs": [],
      "source": [
        "# Check the first ten lines of the combined file\n",
        "!head -n 10 wiki_texts_combined.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfwx92QCEhrq"
      },
      "source": [
        "Please note that we used the default parameters of [`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus) for cleaning the Wiki raw file. Thus, words with one character were discarded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUAzButoP03w"
      },
      "outputs": [],
      "source": [
        "# Now you need to do sampling because the corpus is too big.\n",
        "# You can further perform analysis with a greater sampling ratio.\n",
        "\n",
        "import random\n",
        "\n",
        "wiki_txt_path = \"wiki_texts_combined.txt\"\n",
        "# wiki_texts_combined.txt is a text file separated by linebreaks (\\n).\n",
        "# Each row in wiki_texts_combined.txt indicates a Wikipedia article.\n",
        "\n",
        "with open(wiki_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
        "    # TODO4: Sample `20%` Wikipedia articles\n",
        "    # Write your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7q1Xzunxkdc"
      },
      "outputs": [],
      "source": [
        "# TODO5: Train your own word embeddings with the sampled articles\n",
        "# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
        "# Hint: You should perform some pre-processing before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWiQF70izxP7"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"questions-words.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6xpqgdIy5x1"
      },
      "outputs": [],
      "source": [
        "# Do predictions and preserve the gold answers (word_D)\n",
        "preds = []\n",
        "golds = []\n",
        "\n",
        "for analogy in tqdm(data[\"Question\"]):\n",
        "      # TODO6: Write your code here to use your trained word embeddings for getting predictions of the analogy task.\n",
        "      # You should also preserve the gold answers during iterations for evaluations later.\n",
        "      \"\"\" Hints\n",
        "      # Unpack the analogy (e.g., \"man\", \"woman\", \"king\", \"queen\")\n",
        "      # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d\n",
        "      # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776\n",
        "      # Mikolov et al., 2013: big - biggest and small - smallest\n",
        "      # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).\n",
        "      \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjZ14dQL0mhf"
      },
      "outputs": [],
      "source": [
        "# Collect words from Google Analogy dataset\n",
        "SUB_CATEGORY = \": family\"\n",
        "\n",
        "# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`\n",
        "\n",
        "\n",
        "plt.title(\"Word Relationships from Google Analogy Task\")\n",
        "plt.show()\n",
        "plt.savefig(\"word_relationships.png\", bbox_inches=\"tight\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
